{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57771606",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.5.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/1000: 100%|█████████████████████████████████████████| 422/422 [06:33<00:00,  1.07it/s, Loss=1.7458, Acc=0.3267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss: 1.7458, Train Acc: 0.3267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:07<00:00,  6.24it/s, Loss=1.4838, Acc=0.4430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Val Loss: 1.4838, Val Acc: 0.4430, Val F1: 0.4353\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.4430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|█████████████████████████████████████████| 422/422 [06:38<00:00,  1.06it/s, Loss=1.4114, Acc=0.4810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Train Loss: 1.4114, Train Acc: 0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:07<00:00,  6.59it/s, Loss=1.3033, Acc=0.5424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Val Loss: 1.3033, Val Acc: 0.5424, Val F1: 0.5342\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.5424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|█████████████████████████████████████████| 422/422 [06:52<00:00,  1.02it/s, Loss=1.2601, Acc=0.5268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Train Loss: 1.2601, Train Acc: 0.5268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:07<00:00,  6.09it/s, Loss=1.2133, Acc=0.5484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Val Loss: 1.2133, Val Acc: 0.5484, Val F1: 0.5364\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.5484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|█████████████████████████████████████████| 422/422 [07:00<00:00,  1.00it/s, Loss=1.1673, Acc=0.5589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Train Loss: 1.1673, Train Acc: 0.5589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.63it/s, Loss=1.1721, Acc=0.5794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Val Loss: 1.1721, Val Acc: 0.5794, Val F1: 0.5624\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|█████████████████████████████████████████| 422/422 [07:01<00:00,  1.00it/s, Loss=1.1037, Acc=0.5797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Train Loss: 1.1037, Train Acc: 0.5797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.65it/s, Loss=1.1280, Acc=0.6011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Val Loss: 1.1280, Val Acc: 0.6011, Val F1: 0.6020\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|█████████████████████████████████████████| 422/422 [07:06<00:00,  1.01s/it, Loss=1.0533, Acc=0.5965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Train Loss: 1.0533, Train Acc: 0.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.60it/s, Loss=1.0862, Acc=0.5947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Val Loss: 1.0862, Val Acc: 0.5947, Val F1: 0.5865\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|█████████████████████████████████████████| 422/422 [07:19<00:00,  1.04s/it, Loss=1.0018, Acc=0.6108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Train Loss: 1.0018, Train Acc: 0.6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.69it/s, Loss=1.0593, Acc=0.6141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Val Loss: 1.0593, Val Acc: 0.6141, Val F1: 0.6066\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|█████████████████████████████████████████| 422/422 [07:12<00:00,  1.02s/it, Loss=0.9697, Acc=0.6257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Train Loss: 0.9697, Train Acc: 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.42it/s, Loss=1.0415, Acc=0.6344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Val Loss: 1.0415, Val Acc: 0.6344, Val F1: 0.6292\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|█████████████████████████████████████████| 422/422 [07:09<00:00,  1.02s/it, Loss=0.9367, Acc=0.6346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Train Loss: 0.9367, Train Acc: 0.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:08<00:00,  5.68it/s, Loss=1.0393, Acc=0.6271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Val Loss: 1.0393, Val Acc: 0.6271, Val F1: 0.6184\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|████████████████████████████████████████| 422/422 [07:11<00:00,  1.02s/it, Loss=0.9087, Acc=0.6425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Train Loss: 0.9087, Train Acc: 0.6425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.73it/s, Loss=0.9933, Acc=0.6318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Val Loss: 0.9933, Val Acc: 0.6318, Val F1: 0.6251\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|████████████████████████████████████████| 422/422 [07:11<00:00,  1.02s/it, Loss=0.8833, Acc=0.6520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Train Loss: 0.8833, Train Acc: 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.58it/s, Loss=1.0084, Acc=0.6378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Val Loss: 1.0084, Val Acc: 0.6378, Val F1: 0.6249\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|████████████████████████████████████████| 422/422 [07:22<00:00,  1.05s/it, Loss=0.8537, Acc=0.6631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Train Loss: 0.8537, Train Acc: 0.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.51it/s, Loss=0.9894, Acc=0.6361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Val Loss: 0.9894, Val Acc: 0.6361, Val F1: 0.6309\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|████████████████████████████████████████| 422/422 [07:18<00:00,  1.04s/it, Loss=0.8305, Acc=0.6692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Train Loss: 0.8305, Train Acc: 0.6692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.73it/s, Loss=1.0219, Acc=0.6458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Val Loss: 1.0219, Val Acc: 0.6458, Val F1: 0.6302\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|████████████████████████████████████████| 422/422 [06:42<00:00,  1.05it/s, Loss=0.8074, Acc=0.6744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Train Loss: 0.8074, Train Acc: 0.6744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.40it/s, Loss=1.0112, Acc=0.6534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Val Loss: 1.0112, Val Acc: 0.6534, Val F1: 0.6448\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|████████████████████████████████████████| 422/422 [06:35<00:00,  1.07it/s, Loss=0.7812, Acc=0.6843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Train Loss: 0.7812, Train Acc: 0.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.09it/s, Loss=0.9584, Acc=0.6461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Val Loss: 0.9584, Val Acc: 0.6461, Val F1: 0.6419\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|████████████████████████████████████████| 422/422 [06:34<00:00,  1.07it/s, Loss=0.7641, Acc=0.6967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Train Loss: 0.7641, Train Acc: 0.6967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.10it/s, Loss=0.9577, Acc=0.6634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Val Loss: 0.9577, Val Acc: 0.6634, Val F1: 0.6601\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|████████████████████████████████████████| 422/422 [06:50<00:00,  1.03it/s, Loss=0.7433, Acc=0.6998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Train Loss: 0.7433, Train Acc: 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.82it/s, Loss=0.9451, Acc=0.6554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Val Loss: 0.9451, Val Acc: 0.6554, Val F1: 0.6508\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|████████████████████████████████████████| 422/422 [07:10<00:00,  1.02s/it, Loss=0.7225, Acc=0.7085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Train Loss: 0.7225, Train Acc: 0.7085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  5.88it/s, Loss=0.9670, Acc=0.6671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Val Loss: 0.9670, Val Acc: 0.6671, Val F1: 0.6601\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|████████████████████████████████████████| 422/422 [07:14<00:00,  1.03s/it, Loss=0.6957, Acc=0.7184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Train Loss: 0.6957, Train Acc: 0.7184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.78it/s, Loss=0.9352, Acc=0.6674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Val Loss: 0.9352, Val Acc: 0.6674, Val F1: 0.6677\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|████████████████████████████████████████| 422/422 [06:54<00:00,  1.02it/s, Loss=0.6883, Acc=0.7225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Train Loss: 0.6883, Train Acc: 0.7225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.21it/s, Loss=0.9838, Acc=0.6701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Val Loss: 0.9838, Val Acc: 0.6701, Val F1: 0.6682\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|████████████████████████████████████████| 422/422 [06:49<00:00,  1.03it/s, Loss=0.6707, Acc=0.7301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Train Loss: 0.6707, Train Acc: 0.7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.70it/s, Loss=0.9427, Acc=0.6704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Val Loss: 0.9427, Val Acc: 0.6704, Val F1: 0.6655\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|████████████████████████████████████████| 422/422 [07:11<00:00,  1.02s/it, Loss=0.6489, Acc=0.7356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Train Loss: 0.6489, Train Acc: 0.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.67it/s, Loss=0.9682, Acc=0.6691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Val Loss: 0.9682, Val Acc: 0.6691, Val F1: 0.6631\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|████████████████████████████████████████| 422/422 [07:04<00:00,  1.01s/it, Loss=0.6381, Acc=0.7460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Train Loss: 0.6381, Train Acc: 0.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.24it/s, Loss=0.9568, Acc=0.6688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Val Loss: 0.9568, Val Acc: 0.6688, Val F1: 0.6630\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|████████████████████████████████████████| 422/422 [07:07<00:00,  1.01s/it, Loss=0.6119, Acc=0.7552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: Train Loss: 0.6119, Train Acc: 0.7552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.54it/s, Loss=1.0098, Acc=0.6621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: Val Loss: 1.0098, Val Acc: 0.6621, Val F1: 0.6494\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|████████████████████████████████████████| 422/422 [07:04<00:00,  1.01s/it, Loss=0.5958, Acc=0.7582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: Train Loss: 0.5958, Train Acc: 0.7582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.35it/s, Loss=0.9663, Acc=0.6678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: Val Loss: 0.9663, Val Acc: 0.6678, Val F1: 0.6630\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: 100%|████████████████████████████████████████| 422/422 [07:09<00:00,  1.02s/it, Loss=0.5530, Acc=0.7813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: Train Loss: 0.5530, Train Acc: 0.7813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:09<00:00,  5.21it/s, Loss=0.9756, Acc=0.6858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: Val Loss: 0.9756, Val Acc: 0.6858, Val F1: 0.6835\n",
      "Current Learning Rate: 3e-06\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: 100%|████████████████████████████████████████| 422/422 [07:07<00:00,  1.01s/it, Loss=0.5376, Acc=0.7877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: Train Loss: 0.5376, Train Acc: 0.7877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.77it/s, Loss=0.9729, Acc=0.6828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: Val Loss: 0.9729, Val Acc: 0.6828, Val F1: 0.6799\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: 100%|████████████████████████████████████████| 422/422 [07:09<00:00,  1.02s/it, Loss=0.5337, Acc=0.7891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: Train Loss: 0.5337, Train Acc: 0.7891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.67it/s, Loss=0.9855, Acc=0.6878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: Val Loss: 0.9855, Val Acc: 0.6878, Val F1: 0.6849\n",
      "Current Learning Rate: 3e-06\n",
      "Best model saved at E://Kuliah//UPI//SEMESTER 8//coba coba\\percobaan11_class-weight_best.pt with val accuracy: 0.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: 100%|████████████████████████████████████████| 422/422 [07:06<00:00,  1.01s/it, Loss=0.5303, Acc=0.7918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: Train Loss: 0.5303, Train Acc: 0.7918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:08<00:00,  5.52it/s, Loss=0.9822, Acc=0.6858]\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: Val Loss: 0.9822, Val Acc: 0.6858, Val F1: 0.6824\n",
      "Current Learning Rate: 3e-06\n",
      "Early stopping triggered at epoch 29!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\AppData\\Local\\Temp\\ipykernel_21604\\616816341.py:619: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"E://Kuliah//UPI//SEMESTER 8//coba coba//percobaan11_class-weight_best.pt\")\n",
      "Testing: 100%|████████████████████████████████████████████████| 53/53 [00:11<00:00,  4.82it/s, Loss=0.8323, Acc=0.6944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8323, Test Acc: 0.6944,Test F1: 0.6909\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 0: 0.6048\n",
      "Class 1: 0.6889\n",
      "Class 2: 0.4416\n",
      "Class 3: 0.8716\n",
      "Class 4: 0.5617\n",
      "Class 5: 0.8444\n",
      "Class 6: 0.7529\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.61      0.60      0.61       458\n",
      "     Class 1       0.76      0.69      0.72        45\n",
      "     Class 2       0.57      0.44      0.50       471\n",
      "     Class 3       0.90      0.87      0.89       872\n",
      "     Class 4       0.61      0.56      0.58       575\n",
      "     Class 5       0.71      0.84      0.77       315\n",
      "     Class 6       0.62      0.75      0.68       595\n",
      "\n",
      "    accuracy                           0.69      3331\n",
      "   macro avg       0.68      0.68      0.68      3331\n",
      "weighted avg       0.69      0.69      0.69      3331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import random\n",
    "from torch import nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Constants\n",
    "SEED = 123\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 1000\n",
    "NUM_CLASSES = 7\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ekstrak label dan piksel\n",
    "        self.labels = self.dataframe['emotion'].values\n",
    "        self.pixels = self.dataframe['pixels'].apply(self.string_to_image).values\n",
    "\n",
    "    def string_to_image(self, pixels_string):\n",
    "        # Konversi string piksel menjadi numpy array dan reshape ke 48x48\n",
    "        pixels = np.array(pixels_string.split(), dtype='float32')\n",
    "        image = pixels.reshape(48, 48)\n",
    "        image = np.expand_dims(image, axis=-1)  # Tambahkan channel dimensi\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.fromarray(image.squeeze().astype('uint8'), mode='L')\n",
    "\n",
    "        # Jika ada transformasi, terapkan ke image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "def create_transforms():\n",
    "    # Create transform pipeline manually\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "        transforms.RandomRotation(10),     # Randomly rotate by 10 degrees\n",
    "        transforms.RandomResizedCrop(\n",
    "            size=IMG_SIZE,  # Output size\n",
    "            scale=(0.8, 1.0)  # Range of the random crop size relative to the input size\n",
    "        ),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.1)),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ]) \n",
    "\n",
    "    # Create transform pipeline manually\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    return train_transforms, test_transforms\n",
    "\n",
    "def load_and_split_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.1, stratify=data['emotion'], random_state=SEED)\n",
    "    data_train, data_val = train_test_split(data_train, test_size=0.1, stratify=data_train['emotion'], random_state=SEED)\n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "def create_datasets(data_train, data_val, data_test, train_transforms, test_transforms):\n",
    "    train_dataset = FERDataset(data_train, transform=train_transforms)\n",
    "    val_dataset = FERDataset(data_val, transform=test_transforms)\n",
    "    test_dataset = FERDataset(data_test, transform=test_transforms)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                             generator=torch.Generator().manual_seed(SEED))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                           generator=torch.Generator().manual_seed(SEED))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                            generator=torch.Generator().manual_seed(SEED))\n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(self.layer_norm1(x)) + x \n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0., # Dropout for attention projection\n",
    "                 mlp_dropout:float=0., # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0., # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__()\n",
    "         \n",
    "        assert img_size % 32 == 0, f\"Image size must be divisible by 32, image size: {img_size}\"\n",
    "        \n",
    "        self.mobilenet = mobilenet_v3_large(pretrained=True).features\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels=960, \n",
    "                                    out_channels=embedding_dim,\n",
    "                                    kernel_size=1)\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        self.num_patches = (img_size // 32) ** 2  # MobileNet reduces spatial size by 32x\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.norm = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        \n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Extract features using MobileNet\n",
    "        features = self.mobilenet(pixel_values)  # Output shape: (batch_size, 1280, H', W')\n",
    "        features = self.projection(features)  # Project to embedding_dim: (batch_size, embedding_dim, H', W')\n",
    "\n",
    "        # Flatten the feature maps into a sequence of tokens\n",
    "        features = features.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embedding_dim)\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((class_token, features), dim=1)  # Shape: (batch_size, num_patches + 1, embedding_dim)\n",
    "\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:, 0]\n",
    "\n",
    "        logits = self.head(cls_token_final)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.epochs_no_improve = 0\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "\n",
    "        if self.epochs_no_improve >= self.patience:\n",
    "            self.early_stop = True\n",
    "\n",
    "        return self.early_stop\n",
    "    \n",
    "class ASAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, eta=0.01, adaptive=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Adaptive SAM (ASAM) optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: Model parameters.\n",
    "            base_optimizer: Base optimizer (e.g., SGD, Adam).\n",
    "            rho: Maximum perturbation radius (default: 0.05).\n",
    "            eta: Learning rate for rho adaptation (default: 0.01).\n",
    "            adaptive: Enable layer-wise adaptive rho (default: True).\n",
    "        \"\"\"\n",
    "        defaults = dict(rho=rho, eta=eta, adaptive=adaptive, **kwargs)\n",
    "        super(ASAM, self).__init__(params, defaults)\n",
    "        \n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.state['step'] = 0\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('rho', rho)\n",
    "            group.setdefault('eta', eta)\n",
    "            group.setdefault('adaptive', adaptive)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        \"\"\"\n",
    "        Perturb parameters adaptively based on layer-wise gradients.\n",
    "        \"\"\"\n",
    "        grad_norm = self._grad_norm()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            scale = group['rho'] / (grad_norm + 1e-12)\n",
    "            \n",
    "            # Layer-wise adaptive rho\n",
    "            if group['adaptive']:\n",
    "                layer_grad_norm = torch.norm(\n",
    "                    torch.stack([torch.norm(p.grad) for p in group['params'] if p.grad is not None]),\n",
    "                    p=2\n",
    "                )\n",
    "                adaptive_rho = group['rho'] * (1 + group['eta'] * layer_grad_norm)\n",
    "                scale = adaptive_rho / (grad_norm + 1e-12)\n",
    "            \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # Save original parameters\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                \n",
    "                # Apply adaptive perturbation\n",
    "                e_w = scale * p.grad\n",
    "                p.add_(e_w)\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        \"\"\"\n",
    "        Update parameters using gradients at perturbed point.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                # Restore original parameters\n",
    "                p.data = self.state[p][\"old_p\"]\n",
    "        \n",
    "        # Base optimizer update\n",
    "        self.base_optimizer.step()\n",
    "        \n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "        \n",
    "        self.state['step'] += 1\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        \"\"\"\n",
    "        Compute L2 norm of gradients across all parameters.\n",
    "        \"\"\"\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                torch.norm(p.grad) if p.grad is not None else torch.tensor(0.)\n",
    "                for group in self.param_groups for p in group[\"params\"]\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        raise NotImplementedError(\"ASAM requires first_step() and second_step().\")\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, class_weight):\n",
    "    # Initialize training utilities\n",
    "    base_optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = ASAM(model.parameters(), base_optimizer, rho=0.05, eta=0.1, adaptive=True)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weight.to(DEVICE))\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0)\n",
    "\n",
    "    # Define path\n",
    "    SAVE_PATH = \"E://Kuliah//UPI//SEMESTER 8//coba coba\"\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    # Initialize lists to store training and validation metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Initialize the best metric for model saving\n",
    "    best_val_accuracy = -float('inf')\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # Second forward-backward pass\n",
    "            criterion(model(inputs), targets).backward()\n",
    "            optimizer.second_step(zero_grad=True)  # Update weights\n",
    "\n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{train_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{correct / total:.4f}\"\n",
    "            })\n",
    "\n",
    "        # Calculate training accuracy and loss\n",
    "        train_accuracy = correct / total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Print training summary\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_targets = []\n",
    "        all_predicted = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} (Validation)\")\n",
    "            for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Update statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                # Collect all targets and predictions for F1-score\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    \"Loss\": f\"{val_loss / (batch_idx + 1):.4f}\",\n",
    "                    \"Acc\": f\"{val_correct / val_total:.4f}\"\n",
    "                })\n",
    "\n",
    "        # Calculate validation accuracy, loss, and F1-score\n",
    "        val_accuracy = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print validation summary\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_accuracy:.4f}, \"\n",
    "              f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Step the learning rate scheduler based on validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Print the current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            model_path = os.path.join(SAVE_PATH, \"percobaan11_class-weight_best.pt\")\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict()\n",
    "            }, model_path)\n",
    "    #         torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Best model saved at {model_path} with val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "#         # Save loss and accuracy plots\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "#         plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "#         plt.title(\"Loss per Epoch\")\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         loss_plot_path = os.path.join(SAVE_PATH, \"percobaan1-hybrid_loss.png\")\n",
    "#         plt.savefig(loss_plot_path)\n",
    "#         plt.close()\n",
    "\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "#         plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "#         plt.title(\"Accuracy per Epoch\")\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         accuracy_plot_path = os.path.join(SAVE_PATH, \"percobaan1-hybrid_accuracy.png\")\n",
    "#         plt.savefig(accuracy_plot_path)\n",
    "#         plt.close()\n",
    "        \n",
    "        if early_stopping(avg_val_loss):\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}!\")\n",
    "            break\n",
    "\n",
    "def evaluate_model(best_model, test_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_targets = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = best_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate test accuracy, loss, and F1-score\n",
    "    test_accuracy = test_correct / test_total\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "    per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    # Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "    class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "    # Print test summary\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "          f\"Test Acc: {test_accuracy:.4f},\"\n",
    "          f\"Test F1: {test_f1:.4f}\")\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for i, acc in enumerate(per_class_accuracy):\n",
    "        print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "    # Data preparation\n",
    "    train_transforms, test_transforms = create_transforms()\n",
    "    data_train, data_val, data_test = load_and_split_data(\"E://Kuliah//UPI//SEMESTER 8//dataset skripsi//fer2013v2_clean.csv\")\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets(data_train, data_val, data_test, train_transforms, test_transforms)\n",
    "    \n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "    class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    train_labels = data_train[\"emotion\"]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    model = ViT(num_classes=len(class_names), \n",
    "                in_channels=3, \n",
    "                patch_size=8, \n",
    "                num_heads=8, \n",
    "                embedding_dim=512, \n",
    "                num_transformer_layers=12)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, class_weights)\n",
    "    \n",
    "    best_model = ViT(num_classes=len(class_names), \n",
    "                     in_channels=3, \n",
    "                     patch_size=8, \n",
    "                     num_heads=8, \n",
    "                     embedding_dim=512, \n",
    "                     num_transformer_layers=12)\n",
    "    best_model = best_model.to(DEVICE)\n",
    "    \n",
    "    checkpoint = torch.load(\"E://Kuliah//UPI//SEMESTER 8//coba coba//percobaan11_class-weight_best.pt\")\n",
    "    best_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    evaluate_model(best_model, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc31209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
