{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f52c85-4ef4-4943-a521-ff740da659ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85719059-ed85-4e86-a40a-e26217642235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b309cc-ab32-4ea3-8c91-f1821515f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import random\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77039c05-9f1a-430a-89b0-b22f0d4de2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe35ee-b631-403e-ae59-b7016ed13458",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdddb0d-9e30-4147-938e-a8109e906f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ekstrak label dan piksel\n",
    "        self.labels = self.dataframe['emotion'].values\n",
    "        self.pixels = self.dataframe['pixels'].apply(self.string_to_image).values\n",
    "\n",
    "    def string_to_image(self, pixels_string):\n",
    "        # Konversi string piksel menjadi numpy array dan reshape ke 48x48\n",
    "        pixels = np.array(pixels_string.split(), dtype='float32')\n",
    "        image = pixels.reshape(48, 48)\n",
    "        image = np.expand_dims(image, axis=-1)  # Tambahkan channel dimensi\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.fromarray(image.squeeze().astype('uint8'), mode='L')\n",
    "\n",
    "        # Jika ada transformasi, terapkan ke image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd05f22-3a1d-495e-ab37-5912522b0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train transforms: Compose(\n",
      "    Grayscale(num_output_channels=3)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n",
      "test transforms: Compose(\n",
      "    Grayscale(num_output_channels=3)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate by 10 degrees\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=IMG_SIZE,  # Output size\n",
    "        scale=(0.8, 1.0)  # Range of the random crop size relative to the input size\n",
    "    ),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "]) \n",
    "\n",
    "# Create transform pipeline manually\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    " \n",
    "print(f\"train transforms: {train_transforms}\")\n",
    "print(f\"test transforms: {test_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb51cb0-db73-45d5-b795-d7bbe982676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33303 entries, 0 to 33302\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   pixels   33303 non-null  object\n",
      " 1   emotion  33303 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 520.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('D://Kuliah//UPI//SEMESTER 8//dataset skripsi//fer2013v2_clean.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525f87c-e3d3-4893-9194-b81b2a4b80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 26974\n",
      "Validation set size: 2998\n",
      "Test set size: 3331\n"
     ]
    }
   ],
   "source": [
    "# Pertama, pisahkan data train (90%) dan validation (10%)\n",
    "data_train, data_test = train_test_split(data, test_size=0.1, stratify=data['emotion'], random_state=123)\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.1, stratify=data_train['emotion'], random_state=123)\n",
    "\n",
    "# Cek ukuran masing-masing set untuk memastikan proporsi\n",
    "print(f'Train set size: {len(data_train)}')\n",
    "print(f'Validation set size: {len(data_val)}')\n",
    "print(f'Test set size: {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a112b864-08d8-42cb-a285-994f930f4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERDataset(data_train, transform=train_transforms)\n",
    "val_dataset = FERDataset(data_val, transform=test_transforms)\n",
    "test_dataset = FERDataset(data_test, transform=test_transforms)\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1c38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data_train[\"emotion\"]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[train_labels]\n",
    "\n",
    "# Create a weighted sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c40770a-6a57-48ca-a10f-8a6c3ff485f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Sad': 4, 'Surprise': 5, 'Neutral': 6}\n",
      "id2label: {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Mapping dari label ke ID (label2id)\n",
    "label2id = {c: idx for idx, c in enumerate(labels)}\n",
    "\n",
    "# Mapping dari ID ke label (id2label)\n",
    "id2label = {idx: c for idx, c in enumerate(labels)}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27139b4d-f206-4e4f-82e6-6c26b311b7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the mean and std used for normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Denormalize function\n",
    "def denormalize(tensor, mean, std):\n",
    "    # Clone the tensor to avoid modifying the original\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization: (t * s) + m\n",
    "    return tensor\n",
    "\n",
    "# Ambil satu contoh dari train_dataset\n",
    "image, label = train_dataset[0]  # Index pertama dataset\n",
    "\n",
    "# Denormalize the image\n",
    "image = denormalize(image, mean, std)\n",
    "\n",
    "# Jika transform menghasilkan tensor, konversi ke format numpy\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Clip the values to the valid range [0, 1]\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "# Plot gambar\n",
    "plt.imshow(image, cmap='gray')  # Gunakan cmap='gray' jika gambar grayscale\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Hilangkan sumbu\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b8a0a00-f3ca-46e1-8080-faf95a66d2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09b62-4109-49eb-ae60-3ce50168f049",
   "metadata": {},
   "source": [
    "## Build Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7f5c19-e39f-478b-904a-4070f72d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc468a7-138e-48ca-a495-a91e0988bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76cdbef5-da91-45c7-8847-9de18bcd77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee8d7b1-8603-4ade-8bed-32444a8c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(self.layer_norm1(x)) + x \n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfbe073e-b7c5-44c3-a123-d89a2152a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0., # Dropout for attention projection\n",
    "                 mlp_dropout:float=0., # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0., # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__()\n",
    "         \n",
    "        assert img_size % 32 == 0, f\"Image size must be divisible by 32, image size: {img_size}\"\n",
    "        \n",
    "        self.mobilenet = mobilenet_v3_large(pretrained=True).features\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels=960, \n",
    "                                    out_channels=embedding_dim,\n",
    "                                    kernel_size=1)\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        self.num_patches = (img_size // 32) ** 2  # MobileNet reduces spatial size by 32x\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        \n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Extract features using MobileNet\n",
    "        features = self.mobilenet(pixel_values)  # Output shape: (batch_size, 1280, H', W')\n",
    "        features = self.projection(features)  # Project to embedding_dim: (batch_size, embedding_dim, H', W')\n",
    "\n",
    "        # Flatten the feature maps into a sequence of tokens\n",
    "        features = features.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embedding_dim)\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((class_token, features), dim=1)  # Shape: (batch_size, num_patches + 1, embedding_dim)\n",
    "\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        logits = self.classifier(x[:, 0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbb722-51b9-46af-90e2-4b80265eb878",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b28361-dae6-43fa-b846-69caa47259a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ViT(num_classes=len(class_names), in_channels=3, patch_size=8, num_heads=8, embedding_dim=512, num_transformer_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc2344b3-32fa-4f94-9c56-13b3ac96ee3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape     Output Shape    Param #         Trainable\n",
       "========================================================================================================================\n",
       "ViT (ViT)                                                    [1, 3, 224, 224] [1, 7]          26,112          True\n",
       "Sequential (mobilenet)                                     [1, 3, 224, 224] [1, 960, 7, 7]  --              True\n",
       "    Conv2dNormActivation (0)                              [1, 3, 224, 224] [1, 16, 112, 112] --              True\n",
       "        Conv2d (0)                                       [1, 3, 224, 224] [1, 16, 112, 112] 432             True\n",
       "        BatchNorm2d (1)                                  [1, 16, 112, 112] [1, 16, 112, 112] 32              True\n",
       "        Hardswish (2)                                    [1, 16, 112, 112] [1, 16, 112, 112] --              --\n",
       "    InvertedResidual (1)                                  [1, 16, 112, 112] [1, 16, 112, 112] --              True\n",
       "        Sequential (block)                               [1, 16, 112, 112] [1, 16, 112, 112] 464             True\n",
       "    InvertedResidual (2)                                  [1, 16, 112, 112] [1, 24, 56, 56] --              True\n",
       "        Sequential (block)                               [1, 16, 112, 112] [1, 24, 56, 56] 3,440           True\n",
       "    InvertedResidual (3)                                  [1, 24, 56, 56] [1, 24, 56, 56] --              True\n",
       "        Sequential (block)                               [1, 24, 56, 56] [1, 24, 56, 56] 4,440           True\n",
       "    InvertedResidual (4)                                  [1, 24, 56, 56] [1, 40, 28, 28] --              True\n",
       "        Sequential (block)                               [1, 24, 56, 56] [1, 40, 28, 28] 10,328          True\n",
       "    InvertedResidual (5)                                  [1, 40, 28, 28] [1, 40, 28, 28] --              True\n",
       "        Sequential (block)                               [1, 40, 28, 28] [1, 40, 28, 28] 20,992          True\n",
       "    InvertedResidual (6)                                  [1, 40, 28, 28] [1, 40, 28, 28] --              True\n",
       "        Sequential (block)                               [1, 40, 28, 28] [1, 40, 28, 28] 20,992          True\n",
       "    InvertedResidual (7)                                  [1, 40, 28, 28] [1, 80, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 40, 28, 28] [1, 80, 14, 14] 32,080          True\n",
       "    InvertedResidual (8)                                  [1, 80, 14, 14] [1, 80, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 80, 14, 14] [1, 80, 14, 14] 34,760          True\n",
       "    InvertedResidual (9)                                  [1, 80, 14, 14] [1, 80, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 80, 14, 14] [1, 80, 14, 14] 31,992          True\n",
       "    InvertedResidual (10)                                 [1, 80, 14, 14] [1, 80, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 80, 14, 14] [1, 80, 14, 14] 31,992          True\n",
       "    InvertedResidual (11)                                 [1, 80, 14, 14] [1, 112, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 80, 14, 14] [1, 112, 14, 14] 214,424         True\n",
       "    InvertedResidual (12)                                 [1, 112, 14, 14] [1, 112, 14, 14] --              True\n",
       "        Sequential (block)                               [1, 112, 14, 14] [1, 112, 14, 14] 386,120         True\n",
       "    InvertedResidual (13)                                 [1, 112, 14, 14] [1, 160, 7, 7]  --              True\n",
       "        Sequential (block)                               [1, 112, 14, 14] [1, 160, 7, 7]  429,224         True\n",
       "    InvertedResidual (14)                                 [1, 160, 7, 7]  [1, 160, 7, 7]  --              True\n",
       "        Sequential (block)                               [1, 160, 7, 7]  [1, 160, 7, 7]  797,360         True\n",
       "    InvertedResidual (15)                                 [1, 160, 7, 7]  [1, 160, 7, 7]  --              True\n",
       "        Sequential (block)                               [1, 160, 7, 7]  [1, 160, 7, 7]  797,360         True\n",
       "    Conv2dNormActivation (16)                             [1, 160, 7, 7]  [1, 960, 7, 7]  --              True\n",
       "        Conv2d (0)                                       [1, 160, 7, 7]  [1, 960, 7, 7]  153,600         True\n",
       "        BatchNorm2d (1)                                  [1, 960, 7, 7]  [1, 960, 7, 7]  1,920           True\n",
       "        Hardswish (2)                                    [1, 960, 7, 7]  [1, 960, 7, 7]  --              --\n",
       "Conv2d (projection)                                        [1, 960, 7, 7]  [1, 512, 7, 7]  492,032         True\n",
       "Dropout (embedding_dropout)                                [1, 50, 512]    [1, 50, 512]    --              --\n",
       "Sequential (transformer_encoder)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "    TransformerEncoderBlock (0)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (1)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (2)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (3)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (4)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (5)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (6)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (7)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (8)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (9)                           [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (10)                          [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "    TransformerEncoderBlock (11)                          [1, 50, 512]    [1, 50, 512]    --              True\n",
       "        LayerNorm (layer_norm1)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MultiheadSelfAttentionBlock (msa_block)          [1, 50, 512]    [1, 50, 512]    1,050,624       True\n",
       "        LayerNorm (layer_norm2)                          [1, 50, 512]    [1, 50, 512]    1,024           True\n",
       "        MLPBlock (mlp_block)                             [1, 50, 512]    [1, 50, 512]    3,149,312       True\n",
       "Sequential (classifier)                                    [1, 512]        [1, 7]          --              True\n",
       "    LayerNorm (0)                                         [1, 512]        [1, 512]        1,024           True\n",
       "    Linear (1)                                            [1, 512]        [1, 7]          3,591           True\n",
       "========================================================================================================================\n",
       "Total params: 53,918,519\n",
       "Trainable params: 53,918,519\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 276.04\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 92.76\n",
       "Params size (MB): 165.14\n",
       "Estimated Total Size (MB): 258.50\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(1, 3, 224, 224),  # (batch_size, in_channels, img_size, img_size)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=15,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6556d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('rho', rho)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                p.add_(scale * p.grad)  # Perturbasi parameter\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data = self.state[p][\"old_p\"]  # Kembalikan parameter awal\n",
    "        self.base_optimizer.step()  # Update parameter\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        norm = torch.norm(\n",
    "            torch.stack([\n",
    "                torch.norm(p.grad) for group in self.param_groups for p in group[\"params\"] if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        raise NotImplementedError(\"Gunakan first_step() dan second_step().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ece644-8389-406b-b8e9-802ac5a1f946",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|| 422/422 [06:20<00:00,  1.11it/s, Loss=1.5998, Acc=0.3772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss: 1.5998, Train Acc: 0.3772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.51it/s, Loss=1.3432, Acc=0.4810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Val Loss: 1.3432, Val Acc: 0.4810, Val F1: 0.4713\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|| 422/422 [06:28<00:00,  1.09it/s, Loss=1.2142, Acc=0.5390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Train Loss: 1.2142, Train Acc: 0.5390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.22it/s, Loss=1.2062, Acc=0.5510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Val Loss: 1.2062, Val Acc: 0.5510, Val F1: 0.5502\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|| 422/422 [06:26<00:00,  1.09it/s, Loss=1.0730, Acc=0.5942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Train Loss: 1.0730, Train Acc: 0.5942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.70it/s, Loss=1.1285, Acc=0.5771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Val Loss: 1.1285, Val Acc: 0.5771, Val F1: 0.5755\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|| 422/422 [06:11<00:00,  1.14it/s, Loss=0.9698, Acc=0.6334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Train Loss: 0.9698, Train Acc: 0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.71it/s, Loss=1.0357, Acc=0.6144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Val Loss: 1.0357, Val Acc: 0.6144, Val F1: 0.6047\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|| 422/422 [06:25<00:00,  1.09it/s, Loss=0.9078, Acc=0.6572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Train Loss: 0.9078, Train Acc: 0.6572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.11it/s, Loss=1.0276, Acc=0.6161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Val Loss: 1.0276, Val Acc: 0.6161, Val F1: 0.6154\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|| 422/422 [06:43<00:00,  1.05it/s, Loss=0.8688, Acc=0.6702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Train Loss: 0.8688, Train Acc: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.11it/s, Loss=1.0049, Acc=0.6181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Val Loss: 1.0049, Val Acc: 0.6181, Val F1: 0.6061\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|| 422/422 [06:42<00:00,  1.05it/s, Loss=0.8165, Acc=0.6900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Train Loss: 0.8165, Train Acc: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.15it/s, Loss=0.9814, Acc=0.6318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Val Loss: 0.9814, Val Acc: 0.6318, Val F1: 0.6245\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|| 422/422 [06:39<00:00,  1.06it/s, Loss=0.7817, Acc=0.7071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Train Loss: 0.7817, Train Acc: 0.7071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.22it/s, Loss=0.9741, Acc=0.6331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Val Loss: 0.9741, Val Acc: 0.6331, Val F1: 0.6299\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|| 422/422 [06:42<00:00,  1.05it/s, Loss=0.7585, Acc=0.7161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Train Loss: 0.7585, Train Acc: 0.7161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000 (Validation): 100%|| 47/47 [00:08<00:00,  5.76it/s, Loss=0.9866, Acc=0.6318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Val Loss: 0.9866, Val Acc: 0.6318, Val F1: 0.6258\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|| 422/422 [06:41<00:00,  1.05it/s, Loss=0.7384, Acc=0.7249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Train Loss: 0.7384, Train Acc: 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 (Validation): 100%|| 47/47 [00:08<00:00,  5.79it/s, Loss=0.9471, Acc=0.6448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Val Loss: 0.9471, Val Acc: 0.6448, Val F1: 0.6412\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|| 422/422 [06:44<00:00,  1.04it/s, Loss=0.7036, Acc=0.7337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Train Loss: 0.7036, Train Acc: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.35it/s, Loss=0.9455, Acc=0.6548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Val Loss: 0.9455, Val Acc: 0.6548, Val F1: 0.6550\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|| 422/422 [06:17<00:00,  1.12it/s, Loss=0.6793, Acc=0.7468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Train Loss: 0.6793, Train Acc: 0.7468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.70it/s, Loss=0.9496, Acc=0.6481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Val Loss: 0.9496, Val Acc: 0.6481, Val F1: 0.6479\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|| 422/422 [06:11<00:00,  1.14it/s, Loss=0.6541, Acc=0.7547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Train Loss: 0.6541, Train Acc: 0.7547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000 (Validation): 100%|| 47/47 [00:06<00:00,  6.73it/s, Loss=0.9368, Acc=0.6634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Val Loss: 0.9368, Val Acc: 0.6634, Val F1: 0.6542\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|| 422/422 [06:11<00:00,  1.14it/s, Loss=0.6280, Acc=0.7679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Train Loss: 0.6280, Train Acc: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.69it/s, Loss=0.9172, Acc=0.6658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Val Loss: 0.9172, Val Acc: 0.6658, Val F1: 0.6617\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|| 422/422 [06:10<00:00,  1.14it/s, Loss=0.6021, Acc=0.7791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Train Loss: 0.6021, Train Acc: 0.7791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000 (Validation): 100%|| 47/47 [00:06<00:00,  6.73it/s, Loss=0.9254, Acc=0.6588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Val Loss: 0.9254, Val Acc: 0.6588, Val F1: 0.6573\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|| 422/422 [06:10<00:00,  1.14it/s, Loss=0.5696, Acc=0.7898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Train Loss: 0.5696, Train Acc: 0.7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.37it/s, Loss=0.9071, Acc=0.6778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Val Loss: 0.9071, Val Acc: 0.6778, Val F1: 0.6738\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|| 422/422 [06:24<00:00,  1.10it/s, Loss=0.5509, Acc=0.8027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Train Loss: 0.5509, Train Acc: 0.8027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.46it/s, Loss=0.9548, Acc=0.6614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Val Loss: 0.9548, Val Acc: 0.6614, Val F1: 0.6597\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|| 422/422 [06:20<00:00,  1.11it/s, Loss=0.5169, Acc=0.8138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Train Loss: 0.5169, Train Acc: 0.8138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.47it/s, Loss=0.9231, Acc=0.6698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Val Loss: 0.9231, Val Acc: 0.6698, Val F1: 0.6695\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|| 422/422 [06:24<00:00,  1.10it/s, Loss=0.4915, Acc=0.8246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Train Loss: 0.4915, Train Acc: 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.48it/s, Loss=0.9221, Acc=0.6785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Val Loss: 0.9221, Val Acc: 0.6785, Val F1: 0.6770\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|| 422/422 [06:27<00:00,  1.09it/s, Loss=0.4691, Acc=0.8332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Train Loss: 0.4691, Train Acc: 0.8332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.41it/s, Loss=0.9219, Acc=0.6711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Val Loss: 0.9219, Val Acc: 0.6711, Val F1: 0.6710\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|| 422/422 [06:26<00:00,  1.09it/s, Loss=0.4439, Acc=0.8430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Train Loss: 0.4439, Train Acc: 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.47it/s, Loss=0.9445, Acc=0.6755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Val Loss: 0.9445, Val Acc: 0.6755, Val F1: 0.6731\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|| 422/422 [06:13<00:00,  1.13it/s, Loss=0.4203, Acc=0.8537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Train Loss: 0.4203, Train Acc: 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.47it/s, Loss=0.9411, Acc=0.6785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Val Loss: 0.9411, Val Acc: 0.6785, Val F1: 0.6747\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|| 422/422 [06:17<00:00,  1.12it/s, Loss=0.3882, Acc=0.8671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Train Loss: 0.3882, Train Acc: 0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.17it/s, Loss=0.9233, Acc=0.6828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Val Loss: 0.9233, Val Acc: 0.6828, Val F1: 0.6806\n",
      "Current Learning Rate: 3e-06\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|| 422/422 [06:31<00:00,  1.08it/s, Loss=0.3682, Acc=0.8757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: Train Loss: 0.3682, Train Acc: 0.8757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.15it/s, Loss=0.9210, Acc=0.6821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: Val Loss: 0.9210, Val Acc: 0.6821, Val F1: 0.6811\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|| 422/422 [06:40<00:00,  1.05it/s, Loss=0.3561, Acc=0.8808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: Train Loss: 0.3561, Train Acc: 0.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.17it/s, Loss=0.9318, Acc=0.6845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: Val Loss: 0.9318, Val Acc: 0.6845, Val F1: 0.6821\n",
      "Current Learning Rate: 3e-06\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: 100%|| 422/422 [06:40<00:00,  1.05it/s, Loss=0.3485, Acc=0.8830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: Train Loss: 0.3485, Train Acc: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.12it/s, Loss=0.9338, Acc=0.6848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: Val Loss: 0.9338, Val Acc: 0.6848, Val F1: 0.6837\n",
      "Current Learning Rate: 3e-06\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\backbone_mobilenetv3_balancing_best.pt with val accuracy: 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: 100%|| 422/422 [06:42<00:00,  1.05it/s, Loss=0.3416, Acc=0.8866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: Train Loss: 0.3416, Train Acc: 0.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.03it/s, Loss=0.9295, Acc=0.6835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: Val Loss: 0.9295, Val Acc: 0.6835, Val F1: 0.6819\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: 100%|| 422/422 [06:30<00:00,  1.08it/s, Loss=0.3391, Acc=0.8847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: Train Loss: 0.3391, Train Acc: 0.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.30it/s, Loss=0.9359, Acc=0.6821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: Val Loss: 0.9359, Val Acc: 0.6821, Val F1: 0.6807\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: 100%|| 422/422 [06:32<00:00,  1.07it/s, Loss=0.3320, Acc=0.8886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: Train Loss: 0.3320, Train Acc: 0.8886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.27it/s, Loss=0.9378, Acc=0.6845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: Val Loss: 0.9378, Val Acc: 0.6845, Val F1: 0.6831\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: 100%|| 422/422 [06:35<00:00,  1.07it/s, Loss=0.3380, Acc=0.8890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: Train Loss: 0.3380, Train Acc: 0.8890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.27it/s, Loss=0.9399, Acc=0.6825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: Val Loss: 0.9399, Val Acc: 0.6825, Val F1: 0.6807\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: 100%|| 422/422 [06:34<00:00,  1.07it/s, Loss=0.3291, Acc=0.8930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: Train Loss: 0.3291, Train Acc: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.20it/s, Loss=0.9381, Acc=0.6815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: Val Loss: 0.9381, Val Acc: 0.6815, Val F1: 0.6796\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: 100%|| 422/422 [06:36<00:00,  1.06it/s, Loss=0.3152, Acc=0.8967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: Train Loss: 0.3152, Train Acc: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.25it/s, Loss=0.9393, Acc=0.6811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: Val Loss: 0.9393, Val Acc: 0.6811, Val F1: 0.6796\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: 100%|| 422/422 [06:30<00:00,  1.08it/s, Loss=0.3221, Acc=0.8949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: Train Loss: 0.3221, Train Acc: 0.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.36it/s, Loss=0.9374, Acc=0.6825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: Val Loss: 0.9374, Val Acc: 0.6825, Val F1: 0.6809\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: 100%|| 422/422 [06:28<00:00,  1.09it/s, Loss=0.3300, Acc=0.8907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: Train Loss: 0.3300, Train Acc: 0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.41it/s, Loss=0.9384, Acc=0.6821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: Val Loss: 0.9384, Val Acc: 0.6821, Val F1: 0.6803\n",
      "Current Learning Rate: 3.0000000000000004e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: 100%|| 422/422 [06:28<00:00,  1.09it/s, Loss=0.3267, Acc=0.8911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: Train Loss: 0.3267, Train Acc: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000 (Validation): 100%|| 47/47 [00:07<00:00,  6.33it/s, Loss=0.9386, Acc=0.6815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: Val Loss: 0.9386, Val Acc: 0.6815, Val F1: 0.6798\n",
      "Current Learning Rate: 3.0000000000000004e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:  13%|                                   | 56/422 [00:52<05:39,  1.08it/s, Loss=0.3258, Acc=0.8915]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 52\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Second forward-backward pass\u001b[39;00m\n\u001b[0;32m     55\u001b[0m criterion(model(inputs), targets)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mSAM.first_step\u001b[1;34m(self, zero_grad)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_p\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m---> 19\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perturbasi parameter\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zero_grad:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_CLASSES = 7\n",
    "SEED = 123\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, generator=torch.Generator().manual_seed(SEED), sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "base_optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = SAM(model.parameters(), base_optimizer, rho=0.05)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Define path\n",
    "SAVE_PATH = \"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize the best metric for model saving\n",
    "best_val_accuracy = -float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        # Second forward-backward pass\n",
    "        criterion(model(inputs), targets).backward()\n",
    "        optimizer.second_step(zero_grad=True)  # Update weights\n",
    "\n",
    "        # Update statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{train_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{correct / total:.4f}\"\n",
    "        })\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print training summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_targets = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} (Validation)\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for F1-score\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{val_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{val_correct / val_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate validation accuracy, loss, and F1-score\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Print the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model_path = os.path.join(SAVE_PATH, \"backbone_mobilenetv3_balancing_best.pt\")\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"class_weights\": class_weights\n",
    "        }, model_path)\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model saved at {model_path} with val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    # Save loss and accuracy plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(SAVE_PATH, \"backbone_mobilenetv3_balancing_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    accuracy_plot_path = os.path.join(SAVE_PATH, \"backbone_mobilenetv3_balancing_accuracy.png\")\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9419548-bc46-4909-a041-3c0173fec3c6",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca0091df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model = ViT(num_classes=len(class_names), in_channels=3, patch_size=8, num_heads=8, embedding_dim=512, num_transformer_layers=12)\n",
    "best_model = best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb98608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\AppData\\Local\\Temp\\ipykernel_24804\\1001746634.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen//backbone_mobilenetv3_balancing_best.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen//backbone_mobilenetv3_balancing_best.pt\")\n",
    "best_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d6a4ca-3dfb-4a39-9ea6-ecdca034e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 53/53 [00:08<00:00,  6.19it/s, Loss=0.8906, Acc=0.6941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8906, Test Acc: 0.6941\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 0: 0.5895\n",
      "Class 1: 0.5778\n",
      "Class 2: 0.4586\n",
      "Class 3: 0.8601\n",
      "Class 4: 0.5757\n",
      "Class 5: 0.8222\n",
      "Class 6: 0.7731\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.61      0.59      0.60       458\n",
      "     Class 1       0.84      0.58      0.68        45\n",
      "     Class 2       0.57      0.46      0.51       471\n",
      "     Class 3       0.90      0.86      0.88       872\n",
      "     Class 4       0.58      0.58      0.58       575\n",
      "     Class 5       0.75      0.82      0.78       315\n",
      "     Class 6       0.63      0.77      0.70       595\n",
      "\n",
      "    accuracy                           0.69      3331\n",
      "   macro avg       0.70      0.67      0.68      3331\n",
      "weighted avg       0.70      0.69      0.69      3331\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79252d8-d103-4bf2-82ad-4098293a0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image, mean, std):\n",
    "    \"\"\"\n",
    "    Denormalize a normalized image tensor.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Normalized image tensor (C, H, W).\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized image tensor.\n",
    "    \"\"\"\n",
    "    # Clone the image to avoid modifying the original\n",
    "    image = image.clone()\n",
    "    for c in range(image.shape[0]):\n",
    "        image[c] = image[c] * std[c] + mean[c]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1a78b-c049-45c7-8036-ab6cb3322a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "# Store misclassified images, true labels, and predicted labels\n",
    "misclassified_images = []\n",
    "misclassified_true = []\n",
    "misclassified_pred = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Store misclassified images\n",
    "        misclassified_mask = predicted != targets\n",
    "        if misclassified_mask.any():\n",
    "            misclassified_images.extend(inputs[misclassified_mask].cpu())\n",
    "            misclassified_true.extend(targets[misclassified_mask].cpu().numpy())\n",
    "            misclassified_pred.extend(predicted[misclassified_mask].cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "def plot_misclassified_images(class_id, num_images=5, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Plots misclassified images for a specific class.\n",
    "\n",
    "    Args:\n",
    "        class_id (int): The class ID to visualize misclassifications for.\n",
    "        num_images (int): Number of misclassified images to display.\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    # Filter misclassified images for the specified class\n",
    "    class_misclassified_indices = [i for i, true_label in enumerate(misclassified_true) if true_label == class_id]\n",
    "    class_misclassified_images = [misclassified_images[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_true = [misclassified_true[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_pred = [misclassified_pred[i] for i in class_misclassified_indices]\n",
    "\n",
    "    if not class_misclassified_images:\n",
    "        print(f\"No misclassified images found for Class {class_id}.\")\n",
    "        return\n",
    "\n",
    "    # Limit the number of images to display\n",
    "    num_images = min(num_images, len(class_misclassified_images))\n",
    "\n",
    "    # Plot the misclassified images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        img = class_misclassified_images[i]\n",
    "\n",
    "        # Denormalize the image\n",
    "        img = denormalize(img, mean, std)\n",
    "\n",
    "        # Convert from (C, H, W) to (H, W, C) and clip to valid range\n",
    "        img = img.permute(1, 2, 0)  # Change tensor shape for matplotlib\n",
    "        img = torch.clamp(img, 0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {class_misclassified_true[i]}\\nPred: {class_misclassified_pred[i]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Misclassified Images for Class {class_id}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c42eaa-e139-4ca5-ada7-6b81d34461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "plot_misclassified_images(class_id=6, num_images=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a13bda-a439-47d2-aa33-8a5a5dbd36bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
