{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5509b704",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  88.81 M \n",
      "fwd MACs:                                                               2.1896 GMACs\n",
      "fwd FLOPs:                                                              4.3939 GFLOPS\n",
      "fwd+bwd MACs:                                                           6.5687 GMACs\n",
      "fwd+bwd FLOPs:                                                          13.1818 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ViTMobilenet(\n",
      "  88.81 M = 100% Params, 2.19 GMACs = 100% MACs, 4.39 GFLOPS = 100% FLOPs\n",
      "  (mobilenet): Sequential(\n",
      "    2.97 M = 3.3464% Params, 214.08 MMACs = 9.7774% MACs, 439.88 MFLOPS = 10.0111% FLOPs\n",
      "    (0): Conv2dNormActivation(\n",
      "      464 = 0.0005% Params, 5.42 MMACs = 0.2475% MACs, 11.24 MFLOPS = 0.2558% FLOPs\n",
      "      (0): Conv2d(432 = 0.0005% Params, 5.42 MMACs = 0.2475% MACs, 10.84 MFLOPS = 0.2467% FLOPs, 3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0091% FLOPs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      464 = 0.0005% Params, 5.02 MMACs = 0.2292% MACs, 11.04 MFLOPS = 0.2512% FLOPs\n",
      "      (block): Sequential(\n",
      "        464 = 0.0005% Params, 5.02 MMACs = 0.2292% MACs, 11.04 MFLOPS = 0.2512% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          176 = 0.0002% Params, 1.81 MMACs = 0.0825% MACs, 4.21 MFLOPS = 0.0959% FLOPs\n",
      "          (0): Conv2d(144 = 0.0002% Params, 1.81 MMACs = 0.0825% MACs, 3.61 MFLOPS = 0.0822% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0091% FLOPs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.0046% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          288 = 0.0003% Params, 3.21 MMACs = 0.1467% MACs, 6.82 MFLOPS = 0.1553% FLOPs\n",
      "          (0): Conv2d(256 = 0.0003% Params, 3.21 MMACs = 0.1467% MACs, 6.42 MFLOPS = 0.1462% FLOPs, 16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32 = 0% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0091% FLOPs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      3.44 K = 0.0039% Params, 19.47 MMACs = 0.8891% MACs, 42.1 MFLOPS = 0.9581% FLOPs\n",
      "      (block): Sequential(\n",
      "        3.44 K = 0.0039% Params, 19.47 MMACs = 0.8891% MACs, 42.1 MFLOPS = 0.9581% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          1.15 K = 0.0013% Params, 12.85 MMACs = 0.5867% MACs, 28.1 MFLOPS = 0.6395% FLOPs\n",
      "          (0): Conv2d(1.02 K = 0.0012% Params, 12.85 MMACs = 0.5867% MACs, 25.69 MFLOPS = 0.5847% FLOPs, 16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128 = 0.0001% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0.0365% FLOPs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 802.82 KFLOPS = 0.0183% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          704 = 0.0008% Params, 1.81 MMACs = 0.0825% MACs, 4.21 MFLOPS = 0.0959% FLOPs\n",
      "          (0): Conv2d(576 = 0.0006% Params, 1.81 MMACs = 0.0825% MACs, 3.61 MFLOPS = 0.0822% FLOPs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(128 = 0.0001% Params, 0 MACs = 0% MACs, 401.41 KFLOPS = 0.0091% FLOPs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 200.7 KFLOPS = 0.0046% FLOPs, inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          1.58 K = 0.0018% Params, 4.82 MMACs = 0.22% MACs, 9.78 MFLOPS = 0.2227% FLOPs\n",
      "          (0): Conv2d(1.54 K = 0.0017% Params, 4.82 MMACs = 0.22% MACs, 9.63 MFLOPS = 0.2193% FLOPs, 64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48 = 0.0001% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0.0034% FLOPs, 24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      4.44 K = 0.005% Params, 12.87 MMACs = 0.5878% MACs, 27.25 MFLOPS = 0.6201% FLOPs\n",
      "      (block): Sequential(\n",
      "        4.44 K = 0.005% Params, 12.87 MMACs = 0.5878% MACs, 27.25 MFLOPS = 0.6201% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          1.87 K = 0.0021% Params, 5.42 MMACs = 0.2475% MACs, 11.52 MFLOPS = 0.2621% FLOPs\n",
      "          (0): Conv2d(1.73 K = 0.0019% Params, 5.42 MMACs = 0.2475% MACs, 10.84 MFLOPS = 0.2467% FLOPs, 24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144 = 0.0002% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0.0103% FLOPs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0.0051% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          792 = 0.0009% Params, 2.03 MMACs = 0.0928% MACs, 4.74 MFLOPS = 0.1079% FLOPs\n",
      "          (0): Conv2d(648 = 0.0007% Params, 2.03 MMACs = 0.0928% MACs, 4.06 MFLOPS = 0.0925% FLOPs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(144 = 0.0002% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0.0103% FLOPs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0.0051% FLOPs, inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          1.78 K = 0.002% Params, 5.42 MMACs = 0.2475% MACs, 10.99 MFLOPS = 0.2501% FLOPs\n",
      "          (0): Conv2d(1.73 K = 0.0019% Params, 5.42 MMACs = 0.2475% MACs, 10.84 MFLOPS = 0.2467% FLOPs, 72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48 = 0.0001% Params, 0 MACs = 0% MACs, 150.53 KFLOPS = 0.0034% FLOPs, 24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      10.33 K = 0.0116% Params, 9.09 MMACs = 0.4152% MACs, 19.15 MFLOPS = 0.4358% FLOPs\n",
      "      (block): Sequential(\n",
      "        10.33 K = 0.0116% Params, 9.09 MMACs = 0.4152% MACs, 19.15 MFLOPS = 0.4358% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          1.87 K = 0.0021% Params, 5.42 MMACs = 0.2475% MACs, 11.52 MFLOPS = 0.2621% FLOPs\n",
      "          (0): Conv2d(1.73 K = 0.0019% Params, 5.42 MMACs = 0.2475% MACs, 10.84 MFLOPS = 0.2467% FLOPs, 24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144 = 0.0002% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0.0103% FLOPs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0.0051% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          1.94 K = 0.0022% Params, 1.41 MMACs = 0.0645% MACs, 2.99 MFLOPS = 0.0681% FLOPs\n",
      "          (0): Conv2d(1.8 K = 0.002% Params, 1.41 MMACs = 0.0645% MACs, 2.82 MFLOPS = 0.0642% FLOPs, 72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(144 = 0.0002% Params, 0 MACs = 0% MACs, 112.9 KFLOPS = 0.0026% FLOPs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 56.45 KFLOPS = 0.0013% FLOPs, inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          3.55 K = 0.004% Params, 3.46 KMACs = 0.0002% MACs, 63.48 KFLOPS = 0.0014% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 56.45 KFLOPS = 0.0013% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(1.75 K = 0.002% Params, 1.73 KMACs = 0.0001% MACs, 3.48 KFLOPS = 0.0001% FLOPs, 72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(1.8 K = 0.002% Params, 1.73 KMACs = 0.0001% MACs, 3.53 KFLOPS = 0.0001% FLOPs, 24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 24 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          2.96 K = 0.0033% Params, 2.26 MMACs = 0.1031% MACs, 4.58 MFLOPS = 0.1042% FLOPs\n",
      "          (0): Conv2d(2.88 K = 0.0032% Params, 2.26 MMACs = 0.1031% MACs, 4.52 MFLOPS = 0.1028% FLOPs, 72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80 = 0.0001% Params, 0 MACs = 0% MACs, 62.72 KFLOPS = 0.0014% FLOPs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      20.99 K = 0.0236% Params, 9.89 MMACs = 0.4515% MACs, 20.49 MFLOPS = 0.4664% FLOPs\n",
      "      (block): Sequential(\n",
      "        20.99 K = 0.0236% Params, 9.89 MMACs = 0.4515% MACs, 20.49 MFLOPS = 0.4664% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          5.04 K = 0.0057% Params, 3.76 MMACs = 0.1719% MACs, 7.81 MFLOPS = 0.1777% FLOPs\n",
      "          (0): Conv2d(4.8 K = 0.0054% Params, 3.76 MMACs = 0.1719% MACs, 7.53 MFLOPS = 0.1713% FLOPs, 40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240 = 0.0003% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          3.24 K = 0.0036% Params, 2.35 MMACs = 0.1074% MACs, 4.99 MFLOPS = 0.1135% FLOPs\n",
      "          (0): Conv2d(3 K = 0.0034% Params, 2.35 MMACs = 0.1074% MACs, 4.7 MFLOPS = 0.1071% FLOPs, 120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(240 = 0.0003% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          7.83 K = 0.0088% Params, 7.68 KMACs = 0.0004% MACs, 109.62 KFLOPS = 0.0025% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(3.87 K = 0.0044% Params, 3.84 KMACs = 0.0002% MACs, 7.71 KFLOPS = 0.0002% FLOPs, 120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(3.96 K = 0.0045% Params, 3.84 KMACs = 0.0002% MACs, 7.8 KFLOPS = 0.0002% FLOPs, 32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          4.88 K = 0.0055% Params, 3.76 MMACs = 0.1719% MACs, 7.59 MFLOPS = 0.1727% FLOPs\n",
      "          (0): Conv2d(4.8 K = 0.0054% Params, 3.76 MMACs = 0.1719% MACs, 7.53 MFLOPS = 0.1713% FLOPs, 120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80 = 0.0001% Params, 0 MACs = 0% MACs, 62.72 KFLOPS = 0.0014% FLOPs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      20.99 K = 0.0236% Params, 9.89 MMACs = 0.4515% MACs, 20.49 MFLOPS = 0.4664% FLOPs\n",
      "      (block): Sequential(\n",
      "        20.99 K = 0.0236% Params, 9.89 MMACs = 0.4515% MACs, 20.49 MFLOPS = 0.4664% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          5.04 K = 0.0057% Params, 3.76 MMACs = 0.1719% MACs, 7.81 MFLOPS = 0.1777% FLOPs\n",
      "          (0): Conv2d(4.8 K = 0.0054% Params, 3.76 MMACs = 0.1719% MACs, 7.53 MFLOPS = 0.1713% FLOPs, 40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240 = 0.0003% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          3.24 K = 0.0036% Params, 2.35 MMACs = 0.1074% MACs, 4.99 MFLOPS = 0.1135% FLOPs\n",
      "          (0): Conv2d(3 K = 0.0034% Params, 2.35 MMACs = 0.1074% MACs, 4.7 MFLOPS = 0.1071% FLOPs, 120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(240 = 0.0003% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          7.83 K = 0.0088% Params, 7.68 KMACs = 0.0004% MACs, 109.62 KFLOPS = 0.0025% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(3.87 K = 0.0044% Params, 3.84 KMACs = 0.0002% MACs, 7.71 KFLOPS = 0.0002% FLOPs, 120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(3.96 K = 0.0045% Params, 3.84 KMACs = 0.0002% MACs, 7.8 KFLOPS = 0.0002% FLOPs, 32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          4.88 K = 0.0055% Params, 3.76 MMACs = 0.1719% MACs, 7.59 MFLOPS = 0.1727% FLOPs\n",
      "          (0): Conv2d(4.8 K = 0.0054% Params, 3.76 MMACs = 0.1719% MACs, 7.53 MFLOPS = 0.1713% FLOPs, 120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80 = 0.0001% Params, 0 MACs = 0% MACs, 62.72 KFLOPS = 0.0014% FLOPs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      32.08 K = 0.0361% Params, 11.71 MMACs = 0.5349% MACs, 23.93 MFLOPS = 0.5446% FLOPs\n",
      "      (block): Sequential(\n",
      "        32.08 K = 0.0361% Params, 11.71 MMACs = 0.5349% MACs, 23.93 MFLOPS = 0.5446% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          10.08 K = 0.0114% Params, 7.53 MMACs = 0.3437% MACs, 15.43 MFLOPS = 0.3511% FLOPs\n",
      "          (0): Conv2d(9.6 K = 0.0108% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480 = 0.0005% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0.0086% FLOPs, 240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          2.64 K = 0.003% Params, 423.36 KMACs = 0.0193% MACs, 940.8 KFLOPS = 0.0214% FLOPs\n",
      "          (0): Conv2d(2.16 K = 0.0024% Params, 423.36 KMACs = 0.0193% MACs, 846.72 KFLOPS = 0.0193% FLOPs, 240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(480 = 0.0005% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          19.36 K = 0.0218% Params, 3.76 MMACs = 0.1719% MACs, 7.56 MFLOPS = 0.172% FLOPs\n",
      "          (0): Conv2d(19.2 K = 0.0216% Params, 3.76 MMACs = 0.1719% MACs, 7.53 MFLOPS = 0.1713% FLOPs, 240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160 = 0.0002% Params, 0 MACs = 0% MACs, 31.36 KFLOPS = 0.0007% FLOPs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      34.76 K = 0.0391% Params, 6.62 MMACs = 0.3026% MACs, 13.44 MFLOPS = 0.3058% FLOPs\n",
      "      (block): Sequential(\n",
      "        34.76 K = 0.0391% Params, 6.62 MMACs = 0.3026% MACs, 13.44 MFLOPS = 0.3058% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          16.4 K = 0.0185% Params, 3.14 MMACs = 0.1432% MACs, 6.35 MFLOPS = 0.1445% FLOPs\n",
      "          (0): Conv2d(16 K = 0.018% Params, 3.14 MMACs = 0.1432% MACs, 6.27 MFLOPS = 0.1427% FLOPs, 80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(400 = 0.0005% Params, 0 MACs = 0% MACs, 78.4 KFLOPS = 0.0018% FLOPs, 200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          2.2 K = 0.0025% Params, 352.8 KMACs = 0.0161% MACs, 784 KFLOPS = 0.0178% FLOPs\n",
      "          (0): Conv2d(1.8 K = 0.002% Params, 352.8 KMACs = 0.0161% MACs, 705.6 KFLOPS = 0.0161% FLOPs, 200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(400 = 0.0005% Params, 0 MACs = 0% MACs, 78.4 KFLOPS = 0.0018% FLOPs, 200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          16.16 K = 0.0182% Params, 3.14 MMACs = 0.1432% MACs, 6.3 MFLOPS = 0.1435% FLOPs\n",
      "          (0): Conv2d(16 K = 0.018% Params, 3.14 MMACs = 0.1432% MACs, 6.27 MFLOPS = 0.1427% FLOPs, 200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160 = 0.0002% Params, 0 MACs = 0% MACs, 31.36 KFLOPS = 0.0007% FLOPs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      31.99 K = 0.036% Params, 6.09 MMACs = 0.2784% MACs, 12.37 MFLOPS = 0.2814% FLOPs\n",
      "      (block): Sequential(\n",
      "        31.99 K = 0.036% Params, 6.09 MMACs = 0.2784% MACs, 12.37 MFLOPS = 0.2814% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          15.09 K = 0.017% Params, 2.89 MMACs = 0.1318% MACs, 5.84 MFLOPS = 0.133% FLOPs\n",
      "          (0): Conv2d(14.72 K = 0.0166% Params, 2.89 MMACs = 0.1318% MACs, 5.77 MFLOPS = 0.1313% FLOPs, 80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(368 = 0.0004% Params, 0 MACs = 0% MACs, 72.13 KFLOPS = 0.0016% FLOPs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          2.02 K = 0.0023% Params, 324.58 KMACs = 0.0148% MACs, 721.28 KFLOPS = 0.0164% FLOPs\n",
      "          (0): Conv2d(1.66 K = 0.0019% Params, 324.58 KMACs = 0.0148% MACs, 649.15 KFLOPS = 0.0148% FLOPs, 184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(368 = 0.0004% Params, 0 MACs = 0% MACs, 72.13 KFLOPS = 0.0016% FLOPs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          14.88 K = 0.0168% Params, 2.89 MMACs = 0.1318% MACs, 5.8 MFLOPS = 0.132% FLOPs\n",
      "          (0): Conv2d(14.72 K = 0.0166% Params, 2.89 MMACs = 0.1318% MACs, 5.77 MFLOPS = 0.1313% FLOPs, 184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160 = 0.0002% Params, 0 MACs = 0% MACs, 31.36 KFLOPS = 0.0007% FLOPs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      31.99 K = 0.036% Params, 6.09 MMACs = 0.2784% MACs, 12.37 MFLOPS = 0.2814% FLOPs\n",
      "      (block): Sequential(\n",
      "        31.99 K = 0.036% Params, 6.09 MMACs = 0.2784% MACs, 12.37 MFLOPS = 0.2814% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          15.09 K = 0.017% Params, 2.89 MMACs = 0.1318% MACs, 5.84 MFLOPS = 0.133% FLOPs\n",
      "          (0): Conv2d(14.72 K = 0.0166% Params, 2.89 MMACs = 0.1318% MACs, 5.77 MFLOPS = 0.1313% FLOPs, 80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(368 = 0.0004% Params, 0 MACs = 0% MACs, 72.13 KFLOPS = 0.0016% FLOPs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          2.02 K = 0.0023% Params, 324.58 KMACs = 0.0148% MACs, 721.28 KFLOPS = 0.0164% FLOPs\n",
      "          (0): Conv2d(1.66 K = 0.0019% Params, 324.58 KMACs = 0.0148% MACs, 649.15 KFLOPS = 0.0148% FLOPs, 184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(368 = 0.0004% Params, 0 MACs = 0% MACs, 72.13 KFLOPS = 0.0016% FLOPs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          14.88 K = 0.0168% Params, 2.89 MMACs = 0.1318% MACs, 5.8 MFLOPS = 0.132% FLOPs\n",
      "          (0): Conv2d(14.72 K = 0.0166% Params, 2.89 MMACs = 0.1318% MACs, 5.77 MFLOPS = 0.1313% FLOPs, 184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160 = 0.0002% Params, 0 MACs = 0% MACs, 31.36 KFLOPS = 0.0007% FLOPs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      214.42 K = 0.2414% Params, 19.03 MMACs = 0.8689% MACs, 38.57 MFLOPS = 0.8777% FLOPs\n",
      "      (block): Sequential(\n",
      "        214.42 K = 0.2414% Params, 19.03 MMACs = 0.8689% MACs, 38.57 MFLOPS = 0.8777% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          39.36 K = 0.0443% Params, 7.53 MMACs = 0.3437% MACs, 15.24 MFLOPS = 0.3469% FLOPs\n",
      "          (0): Conv2d(38.4 K = 0.0432% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960 = 0.0011% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          5.28 K = 0.0059% Params, 846.72 KMACs = 0.0387% MACs, 1.88 MFLOPS = 0.0428% FLOPs\n",
      "          (0): Conv2d(4.32 K = 0.0049% Params, 846.72 KMACs = 0.0387% MACs, 1.69 MFLOPS = 0.0385% FLOPs, 480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(960 = 0.0011% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0.0043% FLOPs, 480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          115.8 K = 0.1304% Params, 115.2 KMACs = 0.0053% MACs, 325.2 KFLOPS = 0.0074% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(57.72 K = 0.065% Params, 57.6 KMACs = 0.0026% MACs, 115.32 KFLOPS = 0.0026% FLOPs, 480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(58.08 K = 0.0654% Params, 57.6 KMACs = 0.0026% MACs, 115.68 KFLOPS = 0.0026% FLOPs, 120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 120 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          53.98 K = 0.0608% Params, 10.54 MMACs = 0.4812% MACs, 21.12 MFLOPS = 0.4806% FLOPs\n",
      "          (0): Conv2d(53.76 K = 0.0605% Params, 10.54 MMACs = 0.4812% MACs, 21.07 MFLOPS = 0.4796% FLOPs, 480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(224 = 0.0003% Params, 0 MACs = 0% MACs, 43.9 KFLOPS = 0.001% FLOPs, 112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      386.12 K = 0.4348% Params, 30.91 MMACs = 1.4119% MACs, 62.53 MFLOPS = 1.4232% FLOPs\n",
      "      (block): Sequential(\n",
      "        386.12 K = 0.4348% Params, 30.91 MMACs = 1.4119% MACs, 62.53 MFLOPS = 1.4232% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          76.61 K = 0.0863% Params, 14.75 MMACs = 0.6737% MACs, 29.77 MFLOPS = 0.6775% FLOPs\n",
      "          (0): Conv2d(75.26 K = 0.0847% Params, 14.75 MMACs = 0.6737% MACs, 29.5 MFLOPS = 0.6715% FLOPs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1.34 K = 0.0015% Params, 0 MACs = 0% MACs, 263.42 KFLOPS = 0.006% FLOPs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          7.39 K = 0.0083% Params, 1.19 MMACs = 0.0541% MACs, 2.63 MFLOPS = 0.06% FLOPs\n",
      "          (0): Conv2d(6.05 K = 0.0068% Params, 1.19 MMACs = 0.0541% MACs, 2.37 MFLOPS = 0.054% FLOPs, 672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(1.34 K = 0.0015% Params, 0 MACs = 0% MACs, 263.42 KFLOPS = 0.006% FLOPs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          226.63 K = 0.2552% Params, 225.79 KMACs = 0.0103% MACs, 584.3 KFLOPS = 0.0133% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 131.71 KFLOPS = 0.003% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(113.06 K = 0.1273% Params, 112.9 KMACs = 0.0052% MACs, 225.96 KFLOPS = 0.0051% FLOPs, 672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(113.57 K = 0.1279% Params, 112.9 KMACs = 0.0052% MACs, 226.46 KFLOPS = 0.0052% FLOPs, 168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 168 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          75.49 K = 0.085% Params, 14.75 MMACs = 0.6737% MACs, 29.55 MFLOPS = 0.6725% FLOPs\n",
      "          (0): Conv2d(75.26 K = 0.0847% Params, 14.75 MMACs = 0.6737% MACs, 29.5 MFLOPS = 0.6715% FLOPs, 672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(224 = 0.0003% Params, 0 MACs = 0% MACs, 43.9 KFLOPS = 0.001% FLOPs, 112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      429.22 K = 0.4833% Params, 21.07 MMACs = 0.9623% MACs, 42.52 MFLOPS = 0.9676% FLOPs\n",
      "      (block): Sequential(\n",
      "        429.22 K = 0.4833% Params, 21.07 MMACs = 0.9623% MACs, 42.52 MFLOPS = 0.9676% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          76.61 K = 0.0863% Params, 14.75 MMACs = 0.6737% MACs, 29.77 MFLOPS = 0.6775% FLOPs\n",
      "          (0): Conv2d(75.26 K = 0.0847% Params, 14.75 MMACs = 0.6737% MACs, 29.5 MFLOPS = 0.6715% FLOPs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1.34 K = 0.0015% Params, 0 MACs = 0% MACs, 263.42 KFLOPS = 0.006% FLOPs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          18.14 K = 0.0204% Params, 823.2 KMACs = 0.0376% MACs, 1.71 MFLOPS = 0.039% FLOPs\n",
      "          (0): Conv2d(16.8 K = 0.0189% Params, 823.2 KMACs = 0.0376% MACs, 1.65 MFLOPS = 0.0375% FLOPs, 672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(1.34 K = 0.0015% Params, 0 MACs = 0% MACs, 65.86 KFLOPS = 0.0015% FLOPs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          226.63 K = 0.2552% Params, 225.79 KMACs = 0.0103% MACs, 485.52 KFLOPS = 0.011% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 32.93 KFLOPS = 0.0007% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(113.06 K = 0.1273% Params, 112.9 KMACs = 0.0052% MACs, 225.96 KFLOPS = 0.0051% FLOPs, 672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(113.57 K = 0.1279% Params, 112.9 KMACs = 0.0052% MACs, 226.46 KFLOPS = 0.0052% FLOPs, 168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 168 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          107.84 K = 0.1214% Params, 5.27 MMACs = 0.2406% MACs, 10.55 MFLOPS = 0.2402% FLOPs\n",
      "          (0): Conv2d(107.52 K = 0.1211% Params, 5.27 MMACs = 0.2406% MACs, 10.54 MFLOPS = 0.2398% FLOPs, 672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320 = 0.0004% Params, 0 MACs = 0% MACs, 15.68 KFLOPS = 0.0004% FLOPs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      797.36 K = 0.8978% Params, 16.69 MMACs = 0.7622% MACs, 33.63 MFLOPS = 0.7654% FLOPs\n",
      "      (block): Sequential(\n",
      "        797.36 K = 0.8978% Params, 16.69 MMACs = 0.7622% MACs, 33.63 MFLOPS = 0.7654% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          155.52 K = 0.1751% Params, 7.53 MMACs = 0.3437% MACs, 15.15 MFLOPS = 0.3447% FLOPs\n",
      "          (0): Conv2d(153.6 K = 0.173% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1.92 K = 0.0022% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          25.92 K = 0.0292% Params, 1.18 MMACs = 0.0537% MACs, 2.45 MFLOPS = 0.0557% FLOPs\n",
      "          (0): Conv2d(24 K = 0.027% Params, 1.18 MMACs = 0.0537% MACs, 2.35 MFLOPS = 0.0535% FLOPs, 960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(1.92 K = 0.0022% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          462 K = 0.5202% Params, 460.8 KMACs = 0.021% MACs, 970.08 KFLOPS = 0.0221% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 47.04 KFLOPS = 0.0011% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(230.64 K = 0.2597% Params, 230.4 KMACs = 0.0105% MACs, 461.04 KFLOPS = 0.0105% FLOPs, 960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(231.36 K = 0.2605% Params, 230.4 KMACs = 0.0105% MACs, 461.76 KFLOPS = 0.0105% FLOPs, 240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 240 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          153.92 K = 0.1733% Params, 7.53 MMACs = 0.3437% MACs, 15.07 MFLOPS = 0.3429% FLOPs\n",
      "          (0): Conv2d(153.6 K = 0.173% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320 = 0.0004% Params, 0 MACs = 0% MACs, 15.68 KFLOPS = 0.0004% FLOPs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      797.36 K = 0.8978% Params, 16.69 MMACs = 0.7622% MACs, 33.63 MFLOPS = 0.7654% FLOPs\n",
      "      (block): Sequential(\n",
      "        797.36 K = 0.8978% Params, 16.69 MMACs = 0.7622% MACs, 33.63 MFLOPS = 0.7654% FLOPs\n",
      "        (0): Conv2dNormActivation(\n",
      "          155.52 K = 0.1751% Params, 7.53 MMACs = 0.3437% MACs, 15.15 MFLOPS = 0.3447% FLOPs\n",
      "          (0): Conv2d(153.6 K = 0.173% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1.92 K = 0.0022% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          25.92 K = 0.0292% Params, 1.18 MMACs = 0.0537% MACs, 2.45 MFLOPS = 0.0557% FLOPs\n",
      "          (0): Conv2d(24 K = 0.027% Params, 1.18 MMACs = 0.0537% MACs, 2.35 MFLOPS = 0.0535% FLOPs, 960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(1.92 K = 0.0022% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          462 K = 0.5202% Params, 460.8 KMACs = 0.021% MACs, 970.08 KFLOPS = 0.0221% FLOPs\n",
      "          (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 47.04 KFLOPS = 0.0011% FLOPs, output_size=1)\n",
      "          (fc1): Conv2d(230.64 K = 0.2597% Params, 230.4 KMACs = 0.0105% MACs, 461.04 KFLOPS = 0.0105% FLOPs, 960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(231.36 K = 0.2605% Params, 230.4 KMACs = 0.0105% MACs, 461.76 KFLOPS = 0.0105% FLOPs, 240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 240 FLOPS = 0% FLOPs)\n",
      "          (scale_activation): Hardsigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          153.92 K = 0.1733% Params, 7.53 MMACs = 0.3437% MACs, 15.07 MFLOPS = 0.3429% FLOPs\n",
      "          (0): Conv2d(153.6 K = 0.173% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320 = 0.0004% Params, 0 MACs = 0% MACs, 15.68 KFLOPS = 0.0004% FLOPs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      155.52 K = 0.1751% Params, 7.53 MMACs = 0.3437% MACs, 15.15 MFLOPS = 0.3447% FLOPs\n",
      "      (0): Conv2d(153.6 K = 0.173% Params, 7.53 MMACs = 0.3437% MACs, 15.05 MFLOPS = 0.3426% FLOPs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1.92 K = 0.0022% Params, 0 MACs = 0% MACs, 94.08 KFLOPS = 0.0021% FLOPs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (projection): Conv2d(738.05 K = 0.831% Params, 36.13 MMACs = 1.65% MACs, 72.29 MFLOPS = 1.6453% FLOPs, 960, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (embedding_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "  (transformer_encoder): Sequential(\n",
      "    85.05 M = 95.7707% Params, 1.94 GMACs = 88.5724% MACs, 3.88 GFLOPS = 88.3433% FLOPs\n",
      "    (0): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 353.89 MMACs = 16.1629% MACs, 708.36 MFLOPS = 16.1213% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 192 KFLOPS = 0.0044% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 117.96 MMACs = 5.3876% MACs, 235.96 MFLOPS = 5.3701% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 117.96 MMACs = 5.3876% MACs, 235.96 MFLOPS = 5.3701% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 192 KFLOPS = 0.0044% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 235.93 MMACs = 10.7752% MACs, 472.01 MFLOPS = 10.7424% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 235.93 MMACs = 10.7752% MACs, 472.01 MFLOPS = 10.7424% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 117.96 MMACs = 5.3876% MACs, 235.93 MFLOPS = 5.3695% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 153.6 KFLOPS = 0.0035% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 117.96 MMACs = 5.3876% MACs, 235.93 MFLOPS = 5.3695% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 290.19 MMACs = 13.2535% MACs, 580.85 MFLOPS = 13.2194% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 157.44 KFLOPS = 0.0036% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 96.73 MMACs = 4.4178% MACs, 193.48 MFLOPS = 4.4034% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 96.73 MMACs = 4.4178% MACs, 193.48 MFLOPS = 4.4034% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 157.44 KFLOPS = 0.0036% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 193.46 MMACs = 8.8357% MACs, 387.05 MFLOPS = 8.8088% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 193.46 MMACs = 8.8357% MACs, 387.05 MFLOPS = 8.8088% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 96.73 MMACs = 4.4178% MACs, 193.46 MFLOPS = 4.403% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 125.95 KFLOPS = 0.0029% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 96.73 MMACs = 4.4178% MACs, 193.46 MFLOPS = 4.403% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 254.8 MMACs = 11.6373% MACs, 510.01 MFLOPS = 11.6072% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 138.24 KFLOPS = 0.0031% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 84.93 MMACs = 3.8791% MACs, 169.88 MFLOPS = 3.8664% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 84.93 MMACs = 3.8791% MACs, 169.88 MFLOPS = 3.8664% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 138.24 KFLOPS = 0.0031% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 169.87 MMACs = 7.7582% MACs, 339.85 MFLOPS = 7.7345% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 169.87 MMACs = 7.7582% MACs, 339.85 MFLOPS = 7.7345% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 84.93 MMACs = 3.8791% MACs, 169.87 MFLOPS = 3.866% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 110.59 KFLOPS = 0.0025% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 84.93 MMACs = 3.8791% MACs, 169.87 MFLOPS = 3.866% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 219.41 MMACs = 10.021% MACs, 439.17 MFLOPS = 9.995% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 119.04 KFLOPS = 0.0027% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 73.14 MMACs = 3.3403% MACs, 146.29 MFLOPS = 3.3293% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 73.14 MMACs = 3.3403% MACs, 146.29 MFLOPS = 3.3293% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 119.04 KFLOPS = 0.0027% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 146.28 MMACs = 6.6806% MACs, 292.65 MFLOPS = 6.6603% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 146.28 MMACs = 6.6806% MACs, 292.65 MFLOPS = 6.6603% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 73.14 MMACs = 3.3403% MACs, 146.28 MFLOPS = 3.3291% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 95.23 KFLOPS = 0.0022% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 73.14 MMACs = 3.3403% MACs, 146.28 MFLOPS = 3.3291% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 191.1 MMACs = 8.7279% MACs, 382.51 MFLOPS = 8.7053% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 103.68 KFLOPS = 0.0024% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 63.7 MMACs = 2.9093% MACs, 127.41 MFLOPS = 2.8997% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 63.7 MMACs = 2.9093% MACs, 127.41 MFLOPS = 2.8997% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 103.68 KFLOPS = 0.0024% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 127.4 MMACs = 5.8186% MACs, 254.89 MFLOPS = 5.8009% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 127.4 MMACs = 5.8186% MACs, 254.89 MFLOPS = 5.8009% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 63.7 MMACs = 2.9093% MACs, 127.4 MFLOPS = 2.8995% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 82.94 KFLOPS = 0.0019% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 63.7 MMACs = 2.9093% MACs, 127.4 MFLOPS = 2.8995% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 169.87 MMACs = 7.7582% MACs, 340 MFLOPS = 7.7381% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 92.16 KFLOPS = 0.0021% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 56.62 MMACs = 2.5861% MACs, 113.25 MFLOPS = 2.5775% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 56.62 MMACs = 2.5861% MACs, 113.25 MFLOPS = 2.5775% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 92.16 KFLOPS = 0.0021% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 113.25 MMACs = 5.1721% MACs, 226.57 MFLOPS = 5.1564% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 113.25 MMACs = 5.1721% MACs, 226.57 MFLOPS = 5.1564% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 56.62 MMACs = 2.5861% MACs, 113.25 MFLOPS = 2.5773% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 73.73 KFLOPS = 0.0017% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 56.62 MMACs = 2.5861% MACs, 113.25 MFLOPS = 2.5773% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 148.64 MMACs = 6.7884% MACs, 297.5 MFLOPS = 6.7708% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 80.64 KFLOPS = 0.0018% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 49.55 MMACs = 2.2628% MACs, 99.1 MFLOPS = 2.2553% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 49.55 MMACs = 2.2628% MACs, 99.1 MFLOPS = 2.2553% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 80.64 KFLOPS = 0.0018% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 99.09 MMACs = 4.5256% MACs, 198.25 MFLOPS = 4.5118% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 99.09 MMACs = 4.5256% MACs, 198.25 MFLOPS = 4.5118% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 49.55 MMACs = 2.2628% MACs, 99.09 MFLOPS = 2.2552% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 64.51 KFLOPS = 0.0015% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 49.55 MMACs = 2.2628% MACs, 99.09 MFLOPS = 2.2552% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 92.01 MMACs = 4.2023% MACs, 184.17 MFLOPS = 4.1914% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 49.92 KFLOPS = 0.0011% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 49.92 KFLOPS = 0.0011% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 61.34 MMACs = 2.8016% MACs, 122.72 MFLOPS = 2.793% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 61.34 MMACs = 2.8016% MACs, 122.72 MFLOPS = 2.793% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 39.94 KFLOPS = 0.0009% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 92.01 MMACs = 4.2023% MACs, 184.17 MFLOPS = 4.1914% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 49.92 KFLOPS = 0.0011% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 49.92 KFLOPS = 0.0011% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 61.34 MMACs = 2.8016% MACs, 122.72 MFLOPS = 2.793% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 61.34 MMACs = 2.8016% MACs, 122.72 MFLOPS = 2.793% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 39.94 KFLOPS = 0.0009% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 30.67 MMACs = 1.4008% MACs, 61.34 MFLOPS = 1.3961% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 49.55 MMACs = 2.2628% MACs, 99.17 MFLOPS = 2.2569% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 26.88 KFLOPS = 0.0006% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 26.88 KFLOPS = 0.0006% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 33.03 MMACs = 1.5085% MACs, 66.08 MFLOPS = 1.5039% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 33.03 MMACs = 1.5085% MACs, 66.08 MFLOPS = 1.5039% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 21.5 KFLOPS = 0.0005% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 49.55 MMACs = 2.2628% MACs, 99.17 MFLOPS = 2.2569% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 26.88 KFLOPS = 0.0006% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 26.88 KFLOPS = 0.0006% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 33.03 MMACs = 1.5085% MACs, 66.08 MFLOPS = 1.5039% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 33.03 MMACs = 1.5085% MACs, 66.08 MFLOPS = 1.5039% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 21.5 KFLOPS = 0.0005% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 16.52 MMACs = 0.7543% MACs, 33.03 MFLOPS = 0.7517% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): TransformerEncoderBlock(\n",
      "      7.09 M = 7.9809% Params, 28.31 MMACs = 1.293% MACs, 56.67 MFLOPS = 1.2897% FLOPs\n",
      "      (layer_norm1): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 15.36 KFLOPS = 0.0003% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (msa_block): MultiheadSelfAttentionBlock(\n",
      "        2.36 M = 2.66% Params, 9.44 MMACs = 0.431% MACs, 18.87 MFLOPS = 0.4296% FLOPs\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          2.36 M = 2.66% Params, 9.44 MMACs = 0.431% MACs, 18.87 MFLOPS = 0.4296% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(590.59 K = 0.665% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 15.36 KFLOPS = 0.0003% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp_block): MLPBlock(\n",
      "        4.72 M = 5.3174% Params, 18.87 MMACs = 0.862% MACs, 37.76 MFLOPS = 0.8594% FLOPs\n",
      "        (mlp): Sequential(\n",
      "          4.72 M = 5.3174% Params, 18.87 MMACs = 0.862% MACs, 37.76 MFLOPS = 0.8594% FLOPs\n",
      "          (0): Linear(2.36 M = 2.66% Params, 9.44 MMACs = 0.431% MACs, 18.87 MFLOPS = 0.4296% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 12.29 KFLOPS = 0.0003% FLOPs, approximate='none')\n",
      "          (2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "          (3): Linear(2.36 M = 2.6574% Params, 9.44 MMACs = 0.431% MACs, 18.87 MFLOPS = 0.4296% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_pools): ModuleList(\n",
      "    (0-11): 12 x TokenPooling(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  )\n",
      "  (norm): LayerNorm(1.54 K = 0.0017% Params, 0 MACs = 0% MACs, 3.84 KFLOPS = 0.0001% FLOPs, (768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(5.38 K = 0.0061% Params, 5.38 KMACs = 0.0002% MACs, 10.75 KFLOPS = 0.0002% FLOPs, in_features=768, out_features=7, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "FLOPs:4.3939 GFLOPS   MACs:2.1896 GMACs   Params:88.8106 M \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v3_large, vit_b_32, mobilenet_v2\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from calflops import calculate_flops\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 123\n",
    "\n",
    "class TokenPooling(nn.Module):\n",
    "    def __init__(self, keep_tokens: int, use_weighted: bool = True):\n",
    "        super().__init__()\n",
    "        self.keep_tokens = keep_tokens\n",
    "        self.use_weighted = use_weighted\n",
    "\n",
    "    def forward(self, x: torch.Tensor, significance: torch.Tensor = None) -> torch.Tensor:\n",
    "        B, N_plus_1, D = x.shape\n",
    "        cls_token, tokens = x[:, :1, :], x[:, 1:, :]  # (B, 1, D), (B, N, D)\n",
    "\n",
    "        if self.keep_tokens >= tokens.shape[1]:\n",
    "            return x  # tidak perlu pooling\n",
    "\n",
    "        if not self.use_weighted:\n",
    "            significance = torch.ones(tokens.shape[:2], device=x.device)\n",
    "\n",
    "        # Ambil top-k token berdasarkan skor\n",
    "        topk_scores, topk_indices = torch.topk(significance, self.keep_tokens, dim=1)  # (B, K)\n",
    "\n",
    "        # Ambil token berdasarkan indeks top-k\n",
    "        B_idx = torch.arange(B, device=x.device).unsqueeze(1).expand(-1, self.keep_tokens)  # (B, K)\n",
    "        pooled_tokens = tokens[B_idx, topk_indices]  # (B, K, D)\n",
    "\n",
    "        return torch.cat([cls_token, pooled_tokens], dim=1)  # (B, K+1, D)\n",
    "\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "        self.attn_weights = None\n",
    "    def forward(self, x):\n",
    "        attn_output, attn_weights = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=True,\n",
    "                                             average_attn_weights=False)\n",
    "        self.attn_weights = attn_weights\n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x_attn, attn_weights = self.msa_block(self.layer_norm1(x))\n",
    "        x = x_attn + x\n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "class ViTMobilenet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0., # Dropout for attention projection\n",
    "                 mlp_dropout:float=0., # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0., # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__()\n",
    "         \n",
    "        assert img_size % 32 == 0, f\"Image size must be divisible by 32, image size: {img_size}\"\n",
    "        \n",
    "        self.mobilenet = mobilenet_v3_large(pretrained=True).features\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels=960, \n",
    "                                    out_channels=embedding_dim,\n",
    "                                    kernel_size=1)\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        self.num_patches = (img_size // 32) ** 2  # MobileNet reduces spatial size by 32x\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "        \n",
    "        self.keep_tokens = [40, 35, 30, 26, 23, 20, 12, 12, 6, 6, 3, 0]\n",
    "        # Tambahkan TokenPooling per layer (jumlah token disesuaikan)\n",
    "        self.token_pools = nn.ModuleList([\n",
    "            TokenPooling(keep_tokens=k, use_weighted=True) for k in self.keep_tokens\n",
    "        ])\n",
    "       \n",
    "        self.norm = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        \n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Extract features using MobileNet\n",
    "        features = self.mobilenet(pixel_values)  # Output shape: (batch_size, 1280, H', W')\n",
    "        features = self.projection(features)  # Project to embedding_dim: (batch_size, embedding_dim, H', W')\n",
    "\n",
    "        # Flatten the feature maps into a sequence of tokens\n",
    "        features = features.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embedding_dim)\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((class_token, features), dim=1)  # Shape: (batch_size, num_patches + 1, embedding_dim)\n",
    "\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        significance_scores = []\n",
    "\n",
    "        for i, block in enumerate(self.transformer_encoder):\n",
    "            x, attn_weights = block(x)\n",
    "            \n",
    "            # Hitung significance score: total attention yang diterima setiap token\n",
    "            score = attn_weights.sum(dim=1).sum(dim=1)[:, 1:]  # shape: (B, N-1)\n",
    "            \n",
    "            significance_scores.append(score)\n",
    "            \n",
    "            if self.token_pools[i].keep_tokens > 0:\n",
    "                x = self.token_pools[i](x, significance=score)\n",
    "            else:\n",
    "                x = x[:, :1, :]  # hanya CLS token\n",
    "\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:, 0]\n",
    "\n",
    "        logits = self.head(cls_token_final)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = ViTMobilenet(num_classes=7, \n",
    "                in_channels=3, \n",
    "                num_heads=12, \n",
    "                embedding_dim=768, \n",
    "                num_transformer_layers=12,\n",
    "                mlp_size=3072)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# model = mobilenet_v3_large(pretrained=True)\n",
    "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 7)\n",
    "# model.to(DEVICE)\n",
    "\n",
    "# model = vit_b_32(pretrained=True)\n",
    "# model.heads.head = nn.Linear(model.heads.head.in_features, 7)\n",
    "# model.to(DEVICE)\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "\n",
    "flops, macs, params = calculate_flops(model=model, \n",
    "                                      input_shape=input_shape,\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e627923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
